{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This file was made in Spyder Editor\n",
    "\n",
    "Created on Sat Mar 17 23:31:45 2019\n",
    "\n",
    "@author: jevon\n",
    "\"\"\"\n",
    "#This is a supervised classification problem.\n",
    "\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "\n",
    "#data visualisation\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "color = sns.color_palette()\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "#plotly\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "\n",
    "#plotly offline\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf #Need to research this.\n",
    "cf.set_config_file(offline=True)\n",
    "import cufflinks\n",
    "cufflinks.go_offline(connected=True)\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "#ML model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#data modeling\n",
    "from sklearn import svm, tree, linear_model, neighbors\n",
    "from sklearn import naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier #Need to research this.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#helpers\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#performance metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve, recall_score, log_loss\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "#misc\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import timeit\n",
    "import string\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from dateutil.parser import parse\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source = pd.read_excel('WA_Fn-UseC_-HR-Employee-Attrition.xlsx', sheet_name = 0) #add path to Excel source file\n",
    "print(\"Shape of dataframe is: {}\".format(df_source.shape))\n",
    "\n",
    "df_human_resources = df_source.copy()\n",
    "\n",
    "df_human_resources.columns\n",
    "\n",
    "df_human_resources.head()\n",
    "\n",
    "df_human_resources.columns.to_series().groupby(df_human_resources.dtypes).groups\n",
    "\n",
    "#Datatypes and missing values\n",
    "df_human_resources.info()\n",
    "\n",
    "#Overview of numerical features\n",
    "df_human_resources.describe()\n",
    "\n",
    "df_human_resources.hist(figsize=(25,25))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overview of features by attribute\n",
    "\n",
    "#Begin Age data\n",
    "(mu, sigma) = norm.fit(df_human_resources.loc[df_human_resources['Attrition'] == 'Yes', 'Age'])\n",
    "print('Ex: average age = {:0.2f} years with standard deviation = {0.2f}' .format(mu, sigma))\n",
    "(mu, sigma) = norm.fit(df_human_resources.loc[df_human_resources['Attrition'] == 'No', 'Age'])\n",
    "print('Current: average age = {:0.2f} years with standard deviation = {0.2f}' .format(mu, sigma))\n",
    "\n",
    "x1 = df_human_resources.loc[df_human_resources['Attrition'] == 'No', 'Age']\n",
    "x2 = df_human_resources.loc[df_human_resources['Attrition'] == 'Yes', 'Age']\n",
    "\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active', 'Inactive']\n",
    "\n",
    "fig = ff.create_distplot(hist_data, group_labels, curve_type = 'kde', show_hist = False, show_rug = False)\n",
    "\n",
    "fig['Layout'].update(title = 'Age Distritbuion by Attrition')\n",
    "fig['Layout'].update(xaxis = dict(range=[10, 60], dticks = 5))\n",
    "\n",
    "py.iplot(fig, filename = 'Distplot with Multiple Datasets')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Educational Background areas\n",
    "\n",
    "df_human_resources['EducationField'].value_counts()\n",
    "\n",
    "\n",
    "df_EducationField = pd.DataFrame(columns=[\"EducationField\", \"% of Leavers\"])\n",
    "i = 0\n",
    "for field in list(df_human_resources['EducationField'].unique()):\n",
    "    ratio = (df_human_resources[(df_human_resources['EducationField']==field)&(df_human_resources['Attrition']==\"Yes\")].shape[0] / df_human_resources[df_human_resources[\"EducationField\"]==field].shape[0])\n",
    "    df_EducationField.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "\n",
    "df_EducationFieldGroup = df_EducationField.groupby(by=\"EducationField\").sum()\n",
    "df_EducationFieldGroup.iplot(kind='bar', title='Leavers by Education field (%)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender distribution\n",
    "\n",
    "df_human_resources['Gender'].value_counts()\n",
    "\n",
    "print(\"Normalised gender distribution of ex-employees in the dataset: Male = {:0.2f}%; Female = {:0.2f}%.\".format((df_human_resources[(df_human_resources['Attrition']==\"Yes\") & (df_human_resources['Gender'] == 'Male')].shape[0] / df_human_resources[df_human_resources['Gender']=='Male'].shape[0])*100, (df_human_resources[(df_human_resources['Attrition']==\"Yes\") & (df_human_resources['Gender'] == 'Female')].shape[0] / df_human_resources[df_human_resources['Gender']=='Female'].shape[0])*100))\n",
    "\n",
    "df_Gender = pd.DataFrame(columns=[\"Gender\", \"% of Leavers\"])\n",
    "i = 0\n",
    "for field in list(df_human_resources['Gender'].unique()):\n",
    "    ratio = df_human_resources[(df_human_resources['Gender']==field) & df_human_resources(['Attrition']==\"Yes\")].shape[0] / df_human_resources[df_human_resources['Gender'] == field].shape[0]\n",
    "    i += 1\n",
    "\n",
    "df_GenderGroup = df_Gender.groupby(by = \"Gender\").sum()\n",
    "df_GenderGroup.iplot(kind = 'bar', title = 'Leavers by Gender (%)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Marital Status\n",
    "\n",
    "df_human_resources['MartalStatus'].value_counts()\n",
    "\n",
    "df_MaritalStatus = pd.DataFrame(columns=[\"MaritalStatus\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_human_resources['MaritalStatus'].unique()):\n",
    "    ratio = df_human_resources[(df_human_resources['MaritalStatus']==field) & (df_human_resources['Attrition']==\"Yes\")].shape[0] / df_human_resources[df_human_resources['MaritalStatus']==field].shape[0]\n",
    "    df_MaritalStatus.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "\n",
    "df_MaritalStatusGroup = df_MaritalStatus.groupby(by=\"MaritalStatus\").sum()\n",
    "df_MaritalStatusGroup.iplot(kind='bar', title='Leavers by Marital Status (%)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance from Home\n",
    "\n",
    "print(\"Distance from home for employees to get to work is from {:0.2f} to {:0.2f} miles.\".format(df_human_resources['DistanceFromHome'].min(), df_human_resources['DistanceFromHome'].max()))\n",
    "\n",
    "print('Average distance from home for currently active employees: {:0.2f} miles and ex-employees: {:0.2f} miles'.format(df_human_resources[df_human_resources['Attrition']=='No']['DistanceFromHome'].mean(), df_human_resources[df_human_resources['Attrition']=='Yes']['DistanceFromHome'].mean()))\n",
    "\n",
    "x1 = df_human_resources.loc[df_human_resources['Attrition']=='No', 'DistanceFromHome']\n",
    "x2 = df_human_resources.loc[df_human_resources['Attrition']=='Yes', 'DistanceFromHome']\n",
    "\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active Employees', 'Non-Active Amployees']\n",
    "\n",
    "fig = ff.create_distplot(hist_data, group_labels, curve_type='kde', show_hist=False, show_rug=False)\n",
    "\n",
    "fig['layout'].update(title='Distance from Home Distribution in Percent by Status')\n",
    "fig['layout'].update(xaxis=dict(range=[0,50], dtick=5))\n",
    "\n",
    "py.iplot(fig, filename='Distplot with Multiple Datasets')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin Department data analysis\n",
    "\n",
    "df_human_resources['Department'].value_counts()\n",
    "\n",
    "df_Department = pd.DataFrame(columns = [\"Department\", \"% of Leavers\"])\n",
    "i=0\n",
    "for field in list(df_human_resources(df_human_resources['Department'].unique())):\n",
    "    ratio = df_human_resources[(df_human_resources['Department']==field) & (df_human_resources['Attrition']==\"Yes\")].shape[0] / df_human_resources[df_human_resources['Department']==field].shape[0]\n",
    "    df_Department.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "\n",
    "df_DepartmentGroup = df_human_resources['Department'].groupby(by=\"Department\").sum()\n",
    "df_DepartmentGroup.iplot(kind='bar', title=\"Leavers by Department (%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency of Travel, Job Roles, Job Level, and Job Involvement\n",
    "\n",
    "df_human_resources['BusinessTravel'].value_counts()\n",
    "\n",
    "df_BusinessTravel = pd.DataFrame(columns=[\"BusinessTravel\", \"% of Leavers\"])\n",
    "i = 0\n",
    "for field in list(df_human_resources['BusinessTravel'].unique()):\n",
    "    ratio = df_human_resources[(df_human_resources['BusinessTravel']==field) & (df_human_resources['Attrition']==\"Yes\")].shape[0] / df_human_resources[df_human_resources['BusinessTravel']==field].shape[0]\n",
    "    df_BusinessTravel.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "\n",
    "df_BusinessTravel_Group = df_BusinessTravel.groupby(by=\"BusinessTravel\").sum()\n",
    "df_BusinessTravel_Group.iplot(kind='bar', title='Leavers by Business Travel (%)')\n",
    "\n",
    "df_human_resources['JobRole'].value_counts()\n",
    "\n",
    "df_JobRole = pd.DataFrame(columns=[\"JobRole\", \"% of Leavers\"])\n",
    "i = 0\n",
    "for field in list(df_human_resources['JobRole'].unique()):\n",
    "    ratio = df_human_resources[(df_human_resources['JobRole']==field) & (df_human_resources['Attrition']==\"Yes\")].shape[0] / df_human_resources[df_human_resources['JobRole']==field].shape[0]\n",
    "    df_JobRole.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "\n",
    "df_JobRole_Group = df_JobRole.groupby(by=\"JobRole\").sum()\n",
    "df_JobRole_Group.iplot(kind='bar', title='Leavers by Job Role (%)')\n",
    "\n",
    "df_human_resources['JobLevel'].value_counts()\n",
    "\n",
    "df_JobLevel = pd.DataFrame(columns=[\"JobLevel\", \"% of Leavers\"])\n",
    "i = 0\n",
    "for field in list(df_human_resources['JobLevel'].unique()):\n",
    "    ratio = df_human_resources[(df_human_resources['JobLevel']==field) & (df_human_resources['Attrition']==\"Yes\")].shape[0] / df_human_resources[df_human_resources['JobLevel']==field].shape[0]\n",
    "    df_JobLevel.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "\n",
    "df_JobLevel_Group = df_JobLevel.groupby(by=\"JobLevel\").sum()\n",
    "df_JobLevel_Group.iplot(kind='bar', title='Leavers by Job Level (%)')\n",
    "\n",
    "df_human_resources['JobInvolvement'].value_counts()\n",
    "\n",
    "df_JobInvolvement = pd.DataFrame(columns=[\"JobInvolvement\", \"% of Leavers\"])\n",
    "i = 0\n",
    "for field in list(df_human_resources['JobInvolvement'].unique()):\n",
    "    ratio = df_human_resources[(df_human_resources['JobInvolvement']==field) & (df_human_resources['Attrition']==\"Yes\")].shape[0] / df_human_resources[df_human_resources['JobInvolvement']==field].shape[0]\n",
    "    df_JobInvolvement.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "\n",
    "df_JobInvolvement_Group = df_JobInvolvement.groupby(by=\"JobInvolvement\").sum()\n",
    "df_JobInvolvement_Group.iplot(kind='bar', title='Leavers by Job Involvement (%)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training incidents\n",
    "\n",
    "print(\"Number of training incidents last year varies from {:0.2f} to {:0.2f} years.\".format(df_human_resources['TrainingTimesLastYear'].min(), df_human_resources['TrainingTimesLastYear'].max()))\n",
    "\n",
    "x1 = df_human_resources.loc[df_human_resources['Attrition'] == 'No', 'TrainingTimesLastYear']\n",
    "x2 = df_human_resources.loc[df_human_resources['Attrition'] == 'Yes', 'TrainingTimesLastYear']\n",
    "\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active', 'Inactive']\n",
    "\n",
    "fig = ff.create_distplot(hist_data, group_labels, curve_type = 'kde', show_hist = False, show_rug = False)\n",
    "\n",
    "fig['Layout'].update(title = 'Distritbuion of Training Times Last Year by Attrition')\n",
    "fig['Layout'].update(xaxis = dict(range=[10, 60], dticks = 5))\n",
    "\n",
    "py.iplot(fig, filename = 'Distplot with Multiple Datasets')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Companies worked prior\n",
    "\n",
    "df_NumberOfCompaniesWorked = pd.DataFrame(columns=[\"NumCompaniesWorked\", \"% of Leavers\"])\n",
    "i = 0\n",
    "for field in list(df_human_resources['NumCompaniesWorked'].unique()):\n",
    "    ratio = df_human_resources[(df_human_resources['NumCompaniesWorked']==field) & (df_human_resources['Attrition']==\"Yes\")].shape[0] / df_human_resources[df_human_resources['NumCompaniesWorked']==field].shape[0]\n",
    "    df_JobLevel.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "\n",
    "df_NumberOfCompaniesWorked_Group = df_NumberOfCompaniesWorked.groupby(by=\"NumCompaniesWorked\").sum()\n",
    "df_NumberOfCompaniesWorked_Group.iplot(kind='bar', title='Leavers by Number of Prior Companies Worked (%)')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Years at Company\n",
    "\n",
    "df_human_resources\n",
    "\n",
    "print(\"The number of years spent at this company varies from  {:0.2f} to {:0.2f}.\".format(df_human_resources['YearsAtCompany'].min(), df_human_resources['YearsAtCompany'].max()))\n",
    "\n",
    "print('Average number of years spent at the company for currently active employees: {:0.2f}, and ex-employees: {:0.2f}'.format(df_human_resources[df_human_resources['Attrition']=='No']['YearsAtCompany'].mean(), df_human_resources[df_human_resources['Attrition']=='Yes']['YearsAtCompany'].mean()))\n",
    "\n",
    "x1 = df_human_resources.loc[df_human_resources['Attrition']=='No', 'YearsAtCompany']\n",
    "x2 = df_human_resources.loc[df_human_resources['Attrition']=='Yes', 'YearsAtCompany']\n",
    "\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active Employees', 'Non-Active Amployees']\n",
    "\n",
    "fig = ff.create_distplot(hist_data, group_labels, curve_type='kde', show_hist=False, show_rug=False)\n",
    "\n",
    "fig['layout'].update(title='Years at Company Distribution in Percent by Status')\n",
    "fig['layout'].update(xaxis=dict(range=[0,50], dtick=5))\n",
    "\n",
    "py.iplot(fig, filename='Distplot with Multiple Datasets')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Years with Current Manager\n",
    "\n",
    "print(\"The number of years spent with the current manager varies from  {:0.2f} to {:0.2f}.\".format(df_human_resources['YearsWithCurrManager'].min(), df_human_resources['YearsWithCurrManager'].max()))\n",
    "\n",
    "print('Average number of years spent with the current manager for currently active employees: {:0.2f}, and ex-employees: {:0.2f}'.format(df_human_resources[df_human_resources['Attrition']=='No']['YearsWithCurrManager'].mean(), df_human_resources[df_human_resources['Attrition']=='Yes']['YearsWithCurrManager'].mean()))\n",
    "\n",
    "x1 = df_human_resources.loc[df_human_resources['Attrition']=='No', 'YearsWithCurrManager']\n",
    "x2 = df_human_resources.loc[df_human_resources['Attrition']=='Yes', 'YearsWithCurrManager']\n",
    "\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active Employees', 'Non-Active Amployees']\n",
    "\n",
    "fig = ff.create_distplot(hist_data, group_labels, curve_type='kde', show_hist=False, show_rug=False)\n",
    "\n",
    "fig['layout'].update(title='Years at Company Distribution in Percent by Status')\n",
    "fig['layout'].update(xaxis=dict(range=[0,50], dtick=5))\n",
    "\n",
    "py.iplot(fig, filename='Distplot with Multiple Datasets')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Work Life Balance\n",
    "\n",
    "df_human_resources['WorkLifeBalance'].value_counts()\n",
    "\n",
    "df_WorkLifeBalance = pd.DataFrame(columns=[\"WorkLifeBalance\", \"% of Leavers\"])\n",
    "i = 0\n",
    "for field in list(df_human_resources['WorkLifeBalance'].unique()):\n",
    "    ratio = df_human_resources[(df_human_resources['WorkLifeBalance']==field) & (df_human_resources['Attrition']==\"Yes\")].shape[0] / df_human_resources[df_human_resources['WorkLifeBalance']==field].shape[0]\n",
    "    df_WorkLifeBalance.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "\n",
    "df_WorkLifeBalance_Group = df_WorkLifeBalance.groupby(by=\"WorkLifeBalance\").sum()\n",
    "df_WorkLifeBalance_Group.iplot(kind='bar', title='Leavers by Work Life Balance(%)')\n",
    "\n",
    "\n",
    "df_human_resources['StandardHours'].value_counts()\n",
    "\n",
    "\n",
    "df_human_resources['OverTIme'].value_counts()\n",
    "\n",
    "df_OverTime = pd.DataFrame(columns=[\"OverTime\", \"% of Leavers\"])\n",
    "i = 0\n",
    "for field in list(df_human_resources['OverTime'].unique()):\n",
    "    ratio = df_human_resources[(df_human_resources['OverTime']==field) & (df_human_resources['Attrition']==\"Yes\")].shape[0] / df_human_resources[df_human_resources['OverTime']==field].shape[0]\n",
    "    df_WorkLifeBalance.loc[i] = (field, ratio*100)\n",
    "    i += 1\n",
    "\n",
    "df_WorkLifeBalance_Group = df_WorkLifeBalance.groupby(by=\"OverTime\").sum()\n",
    "df_WorkLifeBalance_Group.iplot(kind='bar', title='Leavers by Over Time (%)')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compensation information\n",
    "\n",
    "print(\"Employee Hourly Rate ranges from {:0.2f} to {:0.2f} years.\".format(df_human_resources['HourlyRate'].min(), df_human_resources['HourlyRate'].max()))\n",
    "\n",
    "print(\"Employee Daily Rate ranges from {:0.2f} to {:0.2f} years.\".format(df_human_resources['DailyRate'].min(), df_human_resources['DailyRate'].max()))\n",
    "\n",
    "print(\"Employee Monthly Rate ranges from {:0.2f} to {:0.2f} years.\".format(df_human_resources['MonthlyRate'].min(), df_human_resources['MonthlyRate'].max()))\n",
    "\n",
    "print(\"Employee Monthly Income ranges from {:0.2f} to {:0.2f} years.\".format(df_human_resources['MonthlyIncome'].min(), df_human_resources['MonthlyIncome'].max()))\n",
    "\n",
    "x1 = df_human_resources.loc[df_human_resources['Attrition'] == 'No', 'MonthlyIncome']\n",
    "x2 = df_human_resources.loc[df_human_resources['Attrition'] == 'Yes', 'MonthlyIncome']\n",
    "\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active', 'Inactive']\n",
    "\n",
    "fig = ff.create_distplot(hist_data, group_labels, curve_type = 'kde', show_hist = False, show_rug = False)\n",
    "\n",
    "fig['Layout'].update(title = 'Distritbuion of Employee Monthly Income by Attrition')\n",
    "fig['Layout'].update(xaxis = dict(range=[10, 60], dticks = 5))\n",
    "\n",
    "py.iplot(fig, filename = 'Distplot with Multiple Datasets')\n",
    "\n",
    "\n",
    "print(\"Percentage salary hikes range from {:0.2f} to {:0.2f} years.\".format(df_human_resources['PercentSalaryHike'].min(), df_human_resources['PercentSalaryHike'].max()))\n",
    "\n",
    "x1 = df_human_resources.loc[df_human_resources['Attrition'] == 'No', 'PercentSalaryHike']\n",
    "x2 = df_human_resources.loc[df_human_resources['Attrition'] == 'Yes', 'PercentSalaryHike']\n",
    "\n",
    "hist_data = [x1, x2]\n",
    "group_labels = ['Active', 'Inactive']\n",
    "\n",
    "fig = ff.create_distplot(hist_data, group_labels, curve_type = 'kde', show_hist = False, show_rug = False)\n",
    "\n",
    "fig['Layout'].update(title = 'Distritbuion of Salary Hike Percents by Attrition')\n",
    "fig['Layout'].update(xaxis = dict(range=[10, 60], dticks = 5))\n",
    "\n",
    "py.iplot(fig, filename = 'Distplot with Multiple Datasets')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Satisfaction and Performance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Attrition and Correlation\n",
    "df_human_resources['Attrition'].value_counts()\n",
    "\n",
    "print(\"Percentage of Current Employees is {:.1f}% and of Ex-employees is: {:.1f}%\".format(\n",
    "    df_human_resources[df_human_resources['Attrition'] == 'No'].shape[0] / df_human_resources.shape[0]*100, df_human_resources[df_human_resources['Attrition'] == 'Yes'].shape[0] / df_human_resources.shape[0]*100))\n",
    "\n",
    "df_human_resources['Attrition'].iplot(kind='hist', xTitle='Attrition', yTitle='count', title='Attrition Distribution')\n",
    "\n",
    "df_human_resoures_transpose = df_human_resources.copy()\n",
    "df_human_resoures_transpose['Target'] = df_human_resoures_transpose['Attrition'].apply(\n",
    "    lambda x: 0 if x == 'No' else 1)\n",
    "df_human_resoures_transpose = df_human_resoures_transpose.drop(\n",
    "    ['Attrition', 'EmployeeCount', 'EmployeeNumber', 'StandardHours', 'Over18'], axis=1)\n",
    "correlations = df_human_resoures_transpose.corr()['Target'].sort_values()\n",
    "print('Most Positive Correlations: \\n', correlations.tail(5))\n",
    "print('\\nMost Negative Correlations: \\n', correlations.head(5))\n",
    "\n",
    "\n",
    "corr = df_human_resoures_transpose.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corr, vmax=.5, mask=mask, annot=True, fmt='0.2f', linewidths=.2, cmap=\"YlGnBu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding object\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "print(df_human_resources.shape)\n",
    "df_human_resources.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding columns with values <= 2\n",
    "\n",
    "label_encoder_count = 0\n",
    "for col in df_human_resources.columns[1:] :\n",
    "    if df_human_resources[col].dtype == 'object':\n",
    "        if len(list(df_human_resources[col].unique())) <= 2 :\n",
    "            label_encoder.fit(df_human_resources[col])\n",
    "            df_human_resources[col] = label_encoder.transform(df_human_resources[col])\n",
    "            label_encoder_count += 1\n",
    "\n",
    "print('{} columns were label enconded.'.format(label_encoder_count))\n",
    "\n",
    "df_human_resources = pd.get_dummies(df_human_resources, drop_first = True)\n",
    "\n",
    "print(df_human_resources.shape)\n",
    "df_human_resources.head()\n",
    "\n",
    "scale = MinMaxScaler(feature_range=(0, 5))\n",
    "humRes_col = list(df_human_resources.columns)\n",
    "humRes_col.remove('Attrition')\n",
    "for col in humRes_col:\n",
    "    df_human_resources[col] = df_human_resources[col].astype(float)\n",
    "    df_human_resources[[col]] = scale.fit_transform(df_human_resources[col])\n",
    "\n",
    "df_human_resources['Attrition'] = pd.to_numeric(df_human_resources['Attrition'], downcast = 'float')\n",
    "df_human_resources.head()\n",
    "\n",
    "print('Size of fully enconded dataset: {}'.format(df_human_resources.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning the target to a new dataframe and casting as a numerical feature\n",
    "target = df_human_resources['Attrition'].copy()\n",
    "\n",
    "trainX, testX, trainy, testy = train_test_split(df_human_resources, target, test_size = 0.25, random_state = 7, stratify = target)\n",
    "\n",
    "print('Size of trainX dataset: ', trainX.shape)\n",
    "print('Size of trainy dataset: ', trainy.shape)\n",
    "print('Size of testX dataset: ', testX.shape)\n",
    "print('Size of testy dataset: ', testy.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression\n",
    "\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression(solver='liblinear', random_state=7, class_weight='balanced')))\n",
    "models.append(('Random Forest', RandomForestClassifier( n_estimators=100, random_state=7)))\n",
    "models.append(('SVM', SVC(gamma='auto', random_state=7)))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('Decision Tree Classifier', DecisionTreeClassifier(random_state=7)))\n",
    "models.append(('Gaussian NB', GaussianNB()))\n",
    "\n",
    "acc_results = []\n",
    "auc_results = []\n",
    "names = []\n",
    "columns = ['Algorithm', 'ROC AUC Mean', 'ROC AUC STD', \n",
    "       'Accuracy Mean', 'Accuracy STD']\n",
    "df_results = pd.DataFrame(columns=col)\n",
    "i = 0\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "    cross_validation_acc_results = model_selection.cross_val_score(\n",
    "        model, trainX, trainy, cross_validation=kfold, scoring='accuracy')\n",
    "    cross_validation_auc_results = model_selection.cross_val_score(model, trainX, trainy, cross_validation=kfold, scoring='roc_auc')\n",
    "\n",
    "    acc_results.append(cross_validation_acc_results)\n",
    "    auc_results.append(cross_validation_auc_results)\n",
    "    names.append(name)\n",
    "    df_results.loc[i] = [name, round(cross_validation_acc_results.mean()*100, 2), round(cross_validation_auc_results.std()*100, 2), round(cross_validation_acc_results.mean()*100, 2), round(cross_validation_auc_results.std()*100, 2)]\n",
    "    i += 1\n",
    "df_results.sort_values(by=['ROC AUC Mean'], ascending=False)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "fig.suptitle('Algorithm Accuracy Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(acc_results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "fig.suptitle('Algorithm ROC AUC Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(auc_results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "modelCV = LogisticRegression(solver='liblinear', class_weight=\"balanced\", random_state=7)\n",
    "scoring = 'roc_auc'\n",
    "results = model_selection.cross_val_score(\n",
    "    modelCV, trainX, trainy, cross_validation=kfold, scoring=scoring)\n",
    "print(\"AUC score (STD): %.2f (%.2f)\" % (results.mean(), results.std()))\n",
    "\n",
    "\n",
    "param_grid = {'alpha': np.arange(1e-03, 2, 0.01)} #hyperparameters\n",
    "logis_gsch = GridSearchCV(LogisticRegression(solver = 'liblinear', class_weight = 'balanced', random_state = 7), iid = True, return_train_score = True, param_grid = param_grid, scoring = 'roc_auc', cross_validation = 10)\n",
    "\n",
    "logis_grid = logis_gsch.fit(trainX, trainy)\n",
    "logis_gopt = logis_grid.best_estimator_\n",
    "result = logis_gsch.cv_results_\n",
    "\n",
    "print('='*30)\n",
    "print('best: ' + str(logis_gsch.best_estimator_))\n",
    "print('best: ' + str(logis_gsch.best_params_))\n",
    "print('best: ', logis_gsch.best_score_)\n",
    "print('='*30)\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "model_cross_validation = LogisticRegression(solver='liblinear', class_weight=\"balanced\", random_state=7)\n",
    "scoring = 'roc_auc'\n",
    "results = model_selection.cross_val_score(model_cross_validation, trainX, trainy, cv=kfold, scoring=scoring)\n",
    "print(\"AUC score (STD): %.2f (%.2f)\" % (results.mean(), results.std()))\n",
    "\n",
    "logis_gopt.fit(trainX, trainy)\n",
    "probably = logis_gopt.predict_proba(testX)\n",
    "probably = probably[:, 1]\n",
    "log_roc_auc = roc_auc_score(testy, probably)\n",
    "print('AUC: %0.5f' % log_roc_auc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier(class_weight = \"balanced\", random_state=7)\n",
    "param_grid = {'n_estimators': [50, 75, 100, 125, 150, 175], 'min_samples_split':[2,4,6,8,10], 'min_samples_leaf': [1, 2, 3, 4], 'max_depth': [5, 10, 15, 20, 25]}\n",
    "\n",
    "grid_obj = GridSearchCV(random_forest_classifier, iid=True, return_train_score=True, param_grid=param_grid, scoring='roc_auc', cross_validation=10)\n",
    "\n",
    "grid_fit = grid_obj.fit(trainX, trainy)\n",
    "random_forest_optimization = grid_fit.best_estimator_\n",
    "\n",
    "print('='*20)\n",
    "print(\"best params: \" + str(grid_obj.best_estimator_))\n",
    "print(\"best params: \" + str(grid_obj.best_params_))\n",
    "print('best score:', grid_obj.best_score_)\n",
    "print('='*20)\n",
    "\n",
    "\n",
    "importances = random_forest_optimization.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "names = [trainX.columns[i] for i in indices]\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.bar(range(trainX.shape[1]), importances[indices])\n",
    "plt.xticks(range(trainX.shape[1]), names, rotation=90)\n",
    "plt.show()\n",
    "\n",
    "importances = random_forest_optimization.feature_importances_\n",
    "df_paramater_coefficient = pd.DataFrame(columns=['Feature', 'Coefficient'])\n",
    "for i in range(44):\n",
    "    feat = trainX.columns[i]\n",
    "    coeff = importances[i]\n",
    "    df_paramater_coefficient.loc[i] = (feat, coeff)\n",
    "df_paramater_coefficient.sort_values(by='Coefficient', ascending=False, inplace=True)\n",
    "df_paramater_coefficient = df_paramater_coefficient.reset_index(drop=True)\n",
    "df_paramater_coefficient.head(10)\n",
    "\n",
    "confuson_matrix = metrics.confusion_matrix(testy, random_forest_optimization.predict(testX))\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(confuson_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "print('Accuracy of RandomForest Regression Classifier on test set: {:0.2f}'.format(random_forest_optimization.score(testX, testy)*100))\n",
    "random_forest_optimization.fit(trainX, trainy)\n",
    "print(classification_report(testy, random_forest_optimization.predict(testX)))\n",
    "\n",
    "random_forest_optimization.fit(trainX, trainy)\n",
    "probs = random_forest_optimization.predict_proba(testX)\n",
    "probs = probs[:, 1]\n",
    "random_forest_optimization_roc_auc = roc_auc_score(testy, probs)\n",
    "print('AUC score: %0.3f' % random_forest_optimization_roc_auc)\n",
    "\n",
    "\n",
    "ranFor_class = RandomForestClassifier(class_weight = 'balanced', random_state = 7)\n",
    "param_grid = {'estimators': [50, 75, 100, 125, 150, 175, 200], 'min_samp_splt': [2, 4, 6, 8, 10], 'min_samp_lf': [1, 2, 3, 4, 5], 'max_depth': [5, 10, 15, 20, 25, 30]}\n",
    "grid_object = GridSearchCV(ranFor_class, iid = True, return_train_score = True, param_grid = param_grid, scoring = 'roc_auc', cv = 10)\n",
    "\n",
    "fitGrid = grid_object.fit(trainX, trainy)\n",
    "ranFor_opt = fitGrid.best_estimator_\n",
    "\n",
    "print('='*30)\n",
    "print('best: ' + str(grid_object.best_estimator_))\n",
    "print('best: ' + str(grid_object.best_params_))\n",
    "print('best: ', grid_object.best_score_)\n",
    "print('='*30)\n",
    "\n",
    "ranFor_opt.fit(trainX, trainy)\n",
    "probably2 = ranFor_opt.predict_proba(testX)\n",
    "probably2 = probably2[:, 1]\n",
    "ranFor_opt_roc_auc = roc_auc_score(testy, probably2)\n",
    "print('AUC: %0.5f' % ranFor_opt_roc_auc)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(testy, logis_gopt.predict_proba(testX)[:,1])\n",
    "rf_fpr, rf_tpr, rf_thresholds = roc_curve(testy, random_forest_optimization.predict_proba(testX)[:,1])\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Logistic Regression ROC\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % log_roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Random Forest ROC\n",
    "plt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % random_forest_optimization_roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Base Rate ROC\n",
    "plt.plot([0,1], [0,1],label='Base Rate' 'k--')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Graph')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
